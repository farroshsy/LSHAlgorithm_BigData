{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTRtbaxUQS+WOVZTTz5HOJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farroshsy/LSHAlgorithm_BigData/blob/main/LSH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Big Data Assignment 3**\n",
        "## LSH Algorithm\n",
        "---\n",
        "Name: Farros Hilmi Syafei \n",
        "<br>\n",
        "Student ID: 5025201012\n",
        "<br>\n",
        "Class: Big Data A\n",
        "<br>\n",
        "Lecturer: Abdul Munif, S.Kom., M.Sc.\n"
      ],
      "metadata": {
        "id": "hMvw5vTDf8_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Source:\n",
        "1. https://www.uber.com/en-ID/blog/lsh/\n",
        "2. https://stackoverflow.com/questions/56816537/cant-find-kaggle-json-file-in-google-colab\n",
        "3. https://spark.apache.org/docs/latest/api/python/index.html\n",
        "4. https://spark.apache.org/docs/latest/ml-features.html#locality-sensitive-hashing"
      ],
      "metadata": {
        "id": "zuqOj44yh5EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "Ho0lVYjlZ6HU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RklN1KTpZpcE",
        "outputId": "36c7214b-855f-43f6-bbf5-5d7f1621f46d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 11.0.18 2023-01-17\n",
            "OpenJDK Runtime Environment (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1, mixed mode, sharing)\n",
            "Python 3.9.16\n"
          ]
        }
      ],
      "source": [
        "!java --version\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Apache Spark (PySpark)"
      ],
      "metadata": {
        "id": "AZ5ltMnfaLmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYe-HZSVaGoH",
        "outputId": "85564601-9116-4da5-e06a-a454ad6fe580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Apache Spark context"
      ],
      "metadata": {
        "id": "-nDC34UAaRCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Apache Spark SQL\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark Session/Context\n",
        "# We are using local machine with all the CPU cores [*]\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Hello Pyspark\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Check spark session\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WjdZ4YHaS3F",
        "outputId": "c1fdd188-3d15-4a3b-fc7c-4453503be022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f6b1c7ab6d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Mining Task"
      ],
      "metadata": {
        "id": "u-aKjlUlaanK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LSH task always consists of three steps:\n",
        "\n",
        "1. Converting original data into vectors\n",
        "2. Calculate the hash using MinHash algorithm\n",
        "3. Searching the similar pairs using k-Nearest Neighbor, or join algorithm."
      ],
      "metadata": {
        "id": "R1nsWPQ5agHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3UmTpJIajAI",
        "outputId": "0bd80a9b-a28a-4d99-dece-e343e8e8025d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.14)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.25.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PLEASE USE YOUR OWN KEY\n",
        "# Download your own key according to this instruction https://github.com/Kaggle/kaggle-api#api-credentials\n",
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "import json\n",
        "api_token = {\"username\": \"farroshs\",\n",
        "             \"key\": \"9ee46f04b534c8a9111454bc07a80dc0\"}\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBI6hxoUakXe",
        "outputId": "7d45df14-291c-4556-88be-c7e25dde4ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download from https://www.kaggle.com/datasets/urbanbricks/wikipedia-promotional-articles\n",
        "\n",
        "!kaggle datasets download -d urbanbricks/wikipedia-promotional-articles"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30V-oJjteyqj",
        "outputId": "aba16bec-587b-4984-e61b-5745fdc4f0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wikipedia-promotional-articles.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Dataset"
      ],
      "metadata": {
        "id": "Xe59f5v1e0GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip wikipedia-promotional-articles.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQGKcxaofItF",
        "outputId": "d27d724b-85d1-4cc1-eeb2-3930885455ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  wikipedia-promotional-articles.zip\n",
            "replace good.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: good.csv                \n",
            "  inflating: promotional.csv         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTEj5-lTfLVC",
        "outputId": "1898ba79-d288-43cd-a1eb-abf14f7f98ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 783148\n",
            "drwxr-xr-x 1 root root      4096 Mar  9 18:55 .\n",
            "drwxr-xr-x 1 root root      4096 Mar  9 17:25 ..\n",
            "drwxr-xr-x 4 root root      4096 Mar  7 18:12 .config\n",
            "-rw-r--r-- 1 root root 475685227 Oct 27  2019 good.csv\n",
            "-rw-r--r-- 1 root root        64 Mar  9 17:44 kaggle.json\n",
            "-rw-r--r-- 1 root root 115360355 Oct 27  2019 promotional.csv\n",
            "-rw-r--r-- 1 root root        75 Mar  9 18:51 result.csv\n",
            "drwxr-xr-x 1 root root      4096 Mar  7 18:14 sample_data\n",
            "-rw-r--r-- 1 root root 210863294 Mar  9 17:47 wikipedia-promotional-articles.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the dataset"
      ],
      "metadata": {
        "id": "_jq7xqMXfMxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV (promotional.csv)\n",
        "df = spark.read.option(\"header\", True).csv(\"promotional.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FX4G4YHfOyJ",
        "outputId": "217f0c15-0e4b-4115-8f5d-d38678804f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- advert: string (nullable = true)\n",
            " |-- coi: string (nullable = true)\n",
            " |-- fanpov: string (nullable = true)\n",
            " |-- pr: string (nullable = true)\n",
            " |-- resume: string (nullable = true)\n",
            " |-- url: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add an ID for the dataset\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "newsDF = df.withColumn(\"id\", monotonically_increasing_id())\n",
        "newsDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWjHWfdgfRVJ",
        "outputId": "f341211c-40c3-47f7-fae9-11997fc2eb3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+---+------+---+------+--------------------+---+\n",
            "|                text|advert|coi|fanpov| pr|resume|                 url| id|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+\n",
            "|1 Litre no Namida...|     0|  0|     1|  0|     0|https://en.wikipe...|  0|\n",
            "|1DayLater was fre...|     1|  1|     0|  0|     0|https://en.wikipe...|  1|\n",
            "|1E is a privately...|     1|  0|     0|  0|     0|https://en.wikipe...|  2|\n",
            "|1Malaysia pronoun...|     1|  0|     0|  0|     0|https://en.wikipe...|  3|\n",
            "|The Jerusalem Bie...|     1|  0|     0|  0|     0|https://en.wikipe...|  4|\n",
            "|1st Round Enterpr...|     0|  0|     0|  1|     0|https://en.wikipe...|  5|\n",
            "|2ergo is a provid...|     1|  0|     0|  0|     0|https://en.wikipe...|  6|\n",
            "|2N Telekomunikace...|     1|  0|     0|  0|     0|https://en.wikipe...|  7|\n",
            "|A 3D printing mar...|     1|  0|     0|  0|     0|https://en.wikipe...|  8|\n",
            "|3DR is an America...|     1|  1|     0|  0|     0|https://en.wikipe...|  9|\n",
            "|3D Systems, headq...|     1|  0|     0|  0|     0|https://en.wikipe...| 10|\n",
            "|3Delight, is 3D c...|     0|  0|     0|  0|     1|https://en.wikipe...| 11|\n",
            "|3DVIA is a brand ...|     1|  1|     0|  0|     0|https://en.wikipe...| 12|\n",
            "|3i Infotech Ltd e...|     1|  0|     0|  0|     0|https://en.wikipe...| 13|\n",
            "|3logy is the Pino...|     1|  0|     0|  0|     0|https://en.wikipe...| 14|\n",
            "|The 4 Hour Chef T...|     1|  0|     0|  1|     0|https://en.wikipe...| 15|\n",
            "|4Children was a c...|     1|  1|     0|  0|     0|https://en.wikipe...| 16|\n",
            "|If this article d...|     1|  0|     0|  0|     0|https://en.wikipe...| 17|\n",
            "|4G Americas is a ...|     0|  0|     0|  1|     0|https://en.wikipe...| 18|\n",
            "|4MMM identified o...|     1|  0|     0|  0|     0|https://en.wikipe...| 19|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the totals row\n",
        "newsDF.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdY6CCrhfT-d",
        "outputId": "df0e1cad-d8cb-484d-8cdf-56f80c19af01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23837"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Prepare the tokenizer\n",
        "We transform the input into tokenized words"
      ],
      "metadata": {
        "id": "VxcboE-lfXe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the tokenizer\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "# create a tokenizer object to tokenize the text\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "# tokenize the text in the dataframe\n",
        "wordsDF = tokenizer.transform(newsDF)\n",
        "\n",
        "# show the resulting dataframe\n",
        "wordsDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfWEMRkAfaLD",
        "outputId": "d3db5edf-99f1-49cf-8fd7-2ee77846d2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+\n",
            "|                text|advert|coi|fanpov| pr|resume|                 url| id|               words|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+\n",
            "|1 Litre no Namida...|     0|  0|     1|  0|     0|https://en.wikipe...|  0|[1, litre, no, na...|\n",
            "|1DayLater was fre...|     1|  1|     0|  0|     0|https://en.wikipe...|  1|[1daylater, was, ...|\n",
            "|1E is a privately...|     1|  0|     0|  0|     0|https://en.wikipe...|  2|[1e, is, a, priva...|\n",
            "|1Malaysia pronoun...|     1|  0|     0|  0|     0|https://en.wikipe...|  3|[1malaysia, prono...|\n",
            "|The Jerusalem Bie...|     1|  0|     0|  0|     0|https://en.wikipe...|  4|[the, jerusalem, ...|\n",
            "|1st Round Enterpr...|     0|  0|     0|  1|     0|https://en.wikipe...|  5|[1st, round, ente...|\n",
            "|2ergo is a provid...|     1|  0|     0|  0|     0|https://en.wikipe...|  6|[2ergo, is, a, pr...|\n",
            "|2N Telekomunikace...|     1|  0|     0|  0|     0|https://en.wikipe...|  7|[2n, telekomunika...|\n",
            "|A 3D printing mar...|     1|  0|     0|  0|     0|https://en.wikipe...|  8|[a, 3d, printing,...|\n",
            "|3DR is an America...|     1|  1|     0|  0|     0|https://en.wikipe...|  9|[3dr, is, an, ame...|\n",
            "|3D Systems, headq...|     1|  0|     0|  0|     0|https://en.wikipe...| 10|[3d, systems,, he...|\n",
            "|3Delight, is 3D c...|     0|  0|     0|  0|     1|https://en.wikipe...| 11|[3delight,, is, 3...|\n",
            "|3DVIA is a brand ...|     1|  1|     0|  0|     0|https://en.wikipe...| 12|[3dvia, is, a, br...|\n",
            "|3i Infotech Ltd e...|     1|  0|     0|  0|     0|https://en.wikipe...| 13|[3i, infotech, lt...|\n",
            "|3logy is the Pino...|     1|  0|     0|  0|     0|https://en.wikipe...| 14|[3logy, is, the, ...|\n",
            "|The 4 Hour Chef T...|     1|  0|     0|  1|     0|https://en.wikipe...| 15|[the, 4, hour, ch...|\n",
            "|4Children was a c...|     1|  1|     0|  0|     0|https://en.wikipe...| 16|[4children, was, ...|\n",
            "|If this article d...|     1|  0|     0|  0|     0|https://en.wikipe...| 17|[if, this, articl...|\n",
            "|4G Americas is a ...|     0|  0|     0|  1|     0|https://en.wikipe...| 18|[4g, americas, is...|\n",
            "|4MMM identified o...|     1|  0|     0|  0|     0|https://en.wikipe...| 19|[4mmm, identified...|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the dataset\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "# define the size of the vocabulary and the minimum document frequency\n",
        "vocabSize=1000\n",
        "\n",
        "# create a CountVectorizer object and fit it on the tokenized data\n",
        "cvModel = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=vocabSize, minDF=10).fit(wordsDF)\n",
        "\n",
        "# transform the tokenized data into a vectorized format\n",
        "vectorizedDF = cvModel.transform(wordsDF)\n",
        "\n",
        "# show the resulting dataframe\n",
        "vectorizedDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbylRht6fcYr",
        "outputId": "4b45b05e-13c1-445c-dafc-757cd54696d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+--------------------+\n",
            "|                text|advert|coi|fanpov| pr|resume|                 url| id|               words|            features|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+--------------------+\n",
            "|1 Litre no Namida...|     0|  0|     1|  0|     0|https://en.wikipe...|  0|[1, litre, no, na...|(1000,[0,1,2,3,4,...|\n",
            "|1DayLater was fre...|     1|  1|     0|  0|     0|https://en.wikipe...|  1|[1daylater, was, ...|(1000,[0,1,2,3,4,...|\n",
            "|1E is a privately...|     1|  0|     0|  0|     0|https://en.wikipe...|  2|[1e, is, a, priva...|(1000,[0,1,2,3,4,...|\n",
            "|1Malaysia pronoun...|     1|  0|     0|  0|     0|https://en.wikipe...|  3|[1malaysia, prono...|(1000,[0,1,2,3,4,...|\n",
            "|The Jerusalem Bie...|     1|  0|     0|  0|     0|https://en.wikipe...|  4|[the, jerusalem, ...|(1000,[0,1,2,3,4,...|\n",
            "|1st Round Enterpr...|     0|  0|     0|  1|     0|https://en.wikipe...|  5|[1st, round, ente...|(1000,[0,1,2,3,4,...|\n",
            "|2ergo is a provid...|     1|  0|     0|  0|     0|https://en.wikipe...|  6|[2ergo, is, a, pr...|(1000,[0,1,2,3,4,...|\n",
            "|2N Telekomunikace...|     1|  0|     0|  0|     0|https://en.wikipe...|  7|[2n, telekomunika...|(1000,[0,1,2,3,4,...|\n",
            "|A 3D printing mar...|     1|  0|     0|  0|     0|https://en.wikipe...|  8|[a, 3d, printing,...|(1000,[0,1,2,3,4,...|\n",
            "|3DR is an America...|     1|  1|     0|  0|     0|https://en.wikipe...|  9|[3dr, is, an, ame...|(1000,[0,1,2,3,4,...|\n",
            "|3D Systems, headq...|     1|  0|     0|  0|     0|https://en.wikipe...| 10|[3d, systems,, he...|(1000,[0,1,2,3,4,...|\n",
            "|3Delight, is 3D c...|     0|  0|     0|  0|     1|https://en.wikipe...| 11|[3delight,, is, 3...|(1000,[0,1,2,3,4,...|\n",
            "|3DVIA is a brand ...|     1|  1|     0|  0|     0|https://en.wikipe...| 12|[3dvia, is, a, br...|(1000,[0,1,2,3,4,...|\n",
            "|3i Infotech Ltd e...|     1|  0|     0|  0|     0|https://en.wikipe...| 13|[3i, infotech, lt...|(1000,[0,1,2,3,4,...|\n",
            "|3logy is the Pino...|     1|  0|     0|  0|     0|https://en.wikipe...| 14|[3logy, is, the, ...|(1000,[0,1,2,3,4,...|\n",
            "|The 4 Hour Chef T...|     1|  0|     0|  1|     0|https://en.wikipe...| 15|[the, 4, hour, ch...|(1000,[0,1,2,3,4,...|\n",
            "|4Children was a c...|     1|  1|     0|  0|     0|https://en.wikipe...| 16|[4children, was, ...|(1000,[0,1,2,3,4,...|\n",
            "|If this article d...|     1|  0|     0|  0|     0|https://en.wikipe...| 17|[if, this, articl...|(1000,[0,1,2,3,4,...|\n",
            "|4G Americas is a ...|     0|  0|     0|  1|     0|https://en.wikipe...| 18|[4g, americas, is...|(1000,[0,1,2,3,4,...|\n",
            "|4MMM identified o...|     1|  0|     0|  0|     0|https://en.wikipe...| 19|[4mmm, identified...|(1000,[0,1,2,3,4,...|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fit/train an LSH Model"
      ],
      "metadata": {
        "id": "CAzyq_ATfeaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using MinHashLSH"
      ],
      "metadata": {
        "id": "Xafr6dxGwC4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import MinHashLSH\n",
        "\n",
        "# Define the MinHashLSH model with the desired input and output columns, and number of hash tables\n",
        "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashValues\", numHashTables=3)\n",
        "\n",
        "# Train the model using the vectorized data\n",
        "LSHmodel = mh.fit(vectorizedDF)\n",
        "\n",
        "# Apply the trained LSH model to the vectorized data and show the results\n",
        "LSHmodel.transform(vectorizedDF).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPPI9g5wfimF",
        "outputId": "94f0821e-351a-44b0-aa7e-402e39b11af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "|                text|advert|coi|fanpov| pr|resume|                 url| id|               words|            features|          hashValues|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "|1 Litre no Namida...|     0|  0|     1|  0|     0|https://en.wikipe...|  0|[1, litre, no, na...|(1000,[0,1,2,3,4,...|[[5698517.0], [2....|\n",
            "|1DayLater was fre...|     1|  1|     0|  0|     0|https://en.wikipe...|  1|[1daylater, was, ...|(1000,[0,1,2,3,4,...|[[8.7883328E7], [...|\n",
            "|1E is a privately...|     1|  0|     0|  0|     0|https://en.wikipe...|  2|[1e, is, a, priva...|(1000,[0,1,2,3,4,...|[[2.1223267E7], [...|\n",
            "|1Malaysia pronoun...|     1|  0|     0|  0|     0|https://en.wikipe...|  3|[1malaysia, prono...|(1000,[0,1,2,3,4,...|[[1.3460892E7], [...|\n",
            "|The Jerusalem Bie...|     1|  0|     0|  0|     0|https://en.wikipe...|  4|[the, jerusalem, ...|(1000,[0,1,2,3,4,...|[[3.3093454E7], [...|\n",
            "|1st Round Enterpr...|     0|  0|     0|  1|     0|https://en.wikipe...|  5|[1st, round, ente...|(1000,[0,1,2,3,4,...|[[2.1676516E7], [...|\n",
            "|2ergo is a provid...|     1|  0|     0|  0|     0|https://en.wikipe...|  6|[2ergo, is, a, pr...|(1000,[0,1,2,3,4,...|[[9353080.0], [1....|\n",
            "|2N Telekomunikace...|     1|  0|     0|  0|     0|https://en.wikipe...|  7|[2n, telekomunika...|(1000,[0,1,2,3,4,...|[[3.6748017E7], [...|\n",
            "|A 3D printing mar...|     1|  0|     0|  0|     0|https://en.wikipe...|  8|[a, 3d, printing,...|(1000,[0,1,2,3,4,...|[[9.199114E7], [8...|\n",
            "|3DR is an America...|     1|  1|     0|  0|     0|https://en.wikipe...|  9|[3dr, is, an, ame...|(1000,[0,1,2,3,4,...|[[1.7568704E7], [...|\n",
            "|3D Systems, headq...|     1|  0|     0|  0|     0|https://en.wikipe...| 10|[3d, systems,, he...|(1000,[0,1,2,3,4,...|[[9353080.0], [2....|\n",
            "|3Delight, is 3D c...|     0|  0|     0|  0|     1|https://en.wikipe...| 11|[3delight,, is, 3...|(1000,[0,1,2,3,4,...|[[4.1309078E7], [...|\n",
            "|3DVIA is a brand ...|     1|  1|     0|  0|     0|https://en.wikipe...| 12|[3dvia, is, a, br...|(1000,[0,1,2,3,4,...|[[9353080.0], [83...|\n",
            "|3i Infotech Ltd e...|     1|  0|     0|  0|     0|https://en.wikipe...| 13|[3i, infotech, lt...|(1000,[0,1,2,3,4,...|[[3.3093454E7], [...|\n",
            "|3logy is the Pino...|     1|  0|     0|  0|     0|https://en.wikipe...| 14|[3logy, is, the, ...|(1000,[0,1,2,3,4,...|[[5698517.0], [2....|\n",
            "|The 4 Hour Chef T...|     1|  0|     0|  1|     0|https://en.wikipe...| 15|[the, 4, hour, ch...|(1000,[0,1,2,3,4,...|[[9.199114E7], [1...|\n",
            "|4Children was a c...|     1|  1|     0|  0|     0|https://en.wikipe...| 16|[4children, was, ...|(1000,[0,1,2,3,4,...|[[5245268.0], [1....|\n",
            "|If this article d...|     1|  0|     0|  0|     0|https://en.wikipe...| 17|[if, this, articl...|(1000,[0,1,2,3,4,...|[[3.6748017E7], [...|\n",
            "|4G Americas is a ...|     0|  0|     0|  1|     0|https://en.wikipe...| 18|[4g, americas, is...|(1000,[0,1,2,3,4,...|[[5.3179265E7], [...|\n",
            "|4MMM identified o...|     1|  0|     0|  0|     0|https://en.wikipe...| 19|[4mmm, identified...|(1000,[0,1,2,3,4,...|[[9806329.0], [1....|\n",
            "+--------------------+------+---+------+---+------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Searching the similar pairs/items for a key \"united\" \"states\""
      ],
      "metadata": {
        "id": "i0advAy4fkib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the index of the word \"united\" and \"states\" in the vocabulary\n",
        "print(cvModel.vocabulary.index(\"united\"))\n",
        "print(cvModel.vocabulary.index(\"states\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQvjgM4OfoLk",
        "outputId": "dc5d804e-0d05-40fc-a8a0-fc94998e2710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92\n",
            "198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the input with 2 words into a 1000-size vector\n",
        "# If the words exist in the index, we will give the value 1.0, otherwise 0.0\n",
        "# Final result: key = [0, 0, ..., 1.0, ..., 1.0, 0, ..., 0]\n",
        "from pyspark.ml.linalg import Vectors\n",
        "key = Vectors.sparse(vocabSize, {cvModel.vocabulary.index(\"united\"): 1.0, cvModel.vocabulary.index(\"states\"): 1.0})\n",
        "\n",
        "print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b17VuxMxfpvn",
        "outputId": "00800ed5-711e-4be4-ee43-a4500f117ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000,[92,198],[1.0,1.0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of neighbors\n",
        "k = 40"
      ],
      "metadata": {
        "id": "WGJboAqlhHx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search inside the LSH model that we already trained\n",
        "resultDF = LSHmodel.approxNearestNeighbors(vectorizedDF, key, k)\n",
        "resultDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvD8W23ChCUu",
        "outputId": "339ac785-4c61-4d53-8ea6-1b574d9353cf"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+---+------+---+------+---+---+-----+--------+----------+-------+\n",
            "|text|advert|coi|fanpov| pr|resume|url| id|words|features|hashValues|distCol|\n",
            "+----+------+---+------+---+------+---+---+-----+--------+----------+-------+\n",
            "+----+------+---+------+---+------+---+---+-----+--------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the result into CSV\n",
        "import pandas as pd\n",
        "data = resultDF.toPandas()\n",
        "data.to_csv(\"result.csv\")"
      ],
      "metadata": {
        "id": "ow2yY6KofyLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Result.csv"
      ],
      "metadata": {
        "id": "X_kHfEpAfz4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read the CSV file into a Pandas dataframe\n",
        "df = pd.read_csv('result.csv')\n",
        "\n",
        "# display the first 5 rows of the dataframe\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_RL-HAXf1Wc",
        "outputId": "5622eca8-2ee1-41bc-abe7-8083d282b281"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                               text  advert  coi  \\\n",
            "0           0  Phyllis C. Borzi was the Obama administration'...       0    0   \n",
            "1           1  Metech Recycling is a United States based elec...       1    0   \n",
            "2           2  Redrock Broadcasting is a media company headqu...       1    0   \n",
            "3           3  American Sandwich Great Eats from All 50 State...       1    0   \n",
            "4           4  Airblaster is a clothing company of the United...       1    0   \n",
            "\n",
            "   fanpov  pr  resume                                                url  \\\n",
            "0       0   0       1      https://en.wikipedia.org/wiki/Phyllis%20Borzi   \n",
            "1       0   0       0  https://en.wikipedia.org/wiki/Metech%20Incorpo...   \n",
            "2       0   0       0  https://en.wikipedia.org/wiki/Redrock%20Broadc...   \n",
            "3       0   0       0  https://en.wikipedia.org/wiki/American%20Sandw...   \n",
            "4       0   0       0           https://en.wikipedia.org/wiki/Airblaster   \n",
            "\n",
            "           id                                              words  \\\n",
            "0        2593  ['phyllis', 'c.', 'borzi', 'was', 'the', 'obam...   \n",
            "1  8589935638  ['metech', 'recycling', 'is', 'a', 'united', '...   \n",
            "2  8589939640  ['redrock', 'broadcasting', 'is', 'a', 'media'...   \n",
            "3         790  ['american', 'sandwich', 'great', 'eats', 'fro...   \n",
            "4         421  ['airblaster', 'is', 'a', 'clothing', 'company...   \n",
            "\n",
            "                                            features  \\\n",
            "0  (1000,[0,2,3,7,8,92,194,198,515,593],[4.0,3.0,...   \n",
            "1  (1000,[0,1,2,3,5,6,11,15,16,41,55,59,92,116,19...   \n",
            "2  (1000,[0,3,5,6,41,92,167,309,322,706],[2.0,2.0...   \n",
            "3  (1000,[0,1,2,5,6,10,12,15,18,19,20,27,38,50,92...   \n",
            "4  (1000,[0,1,2,4,5,6,7,10,13,18,19,29,41,50,92,1...   \n",
            "\n",
            "                                          hashValues   distCol  \n",
            "0  [DenseVector([122634346.0]), DenseVector([4468...  0.800000  \n",
            "1  [DenseVector([122634346.0]), DenseVector([1001...  0.882353  \n",
            "2  [DenseVector([122634346.0]), DenseVector([1216...  0.909091  \n",
            "3  [DenseVector([122634346.0]), DenseVector([1216...  0.920000  \n",
            "4  [DenseVector([122634346.0]), DenseVector([1489...  0.920000  \n"
          ]
        }
      ]
    }
  ]
}